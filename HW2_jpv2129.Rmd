---
title: "Homework 2"
author: "Jake Vettoretti"
date: "2025-09-26"
output: github_document
---

```{r setup, echo=FALSE, message=FALSE}
library(tidyverse) #installing necessary libraries
library(readr)
library(readxl)
library(lubridate)
library(knitr)
```

## Problem 1

Installing and cleaning pols data.

```{r}
pols_df = read_csv("data/pols-month.csv") |> #Clean pols.csv data
  janitor::clean_names() |> 
  separate(mon, into = c("year", "month", "day"), sep = "-", convert = TRUE) |> 
  mutate(
    month = month.name[as.numeric(month)],
    president = case_when(
      prez_gop == 1 ~ "gop",
      prez_dem == 1 ~ "dem",
      TRUE ~ NA_character_
    )
  ) |> 
  select(-prez_dem, -prez_gop, -day) |> 
  arrange(year, month)

pols_df
```

Installing and cleaning snp data.

```{r}
snp_df = read_csv("data/snp.csv") |>  # Clean snp.csv data
  janitor::clean_names() |>
  mutate(
    date = as.Date(date, format = "%m/%d/%y"),
    year = as.integer(ifelse(
      format(date, "%y") <= "68", 
      paste0("20", format(date, "%y")),
      paste0("19", format(date, "%y"))
    )),
    month = month.name[as.integer(format(date, "%m"))]
  ) |>
  select(year, month, close) |>
  arrange(year, month)

snp_df
```

Preparing datasets for merge.

```{r}
unemployment_df = read_csv("data/unemployment.csv") |> 
  janitor::clean_names() |> 
  pivot_longer(
    cols = jan:dec,
    names_to = "month_abbr",
    values_to = "unemployment_rate"
  ) |> 
  # Convert abbreviations using month.name and month.abb constants
  mutate(month = month.name[match(month_abbr, tolower(month.abb))]) |> 
  select(-month_abbr) |> 
  rename(year = year) |> 
  arrange(year, month)
```

Merging the data sets

```{r}
merged_data = pols_df |> #merging pols and snp to prepare for merge into unemployment
  left_join(snp_df, by = c("year", "month"))

final_data = merged_data |>  # Step 2: Merge unemployment into the result  
  left_join(unemployment_df, by = c("year", "month"))

final_data
```

Write a short paragraph about these datasets. Explain briefly what each dataset contained, and describe the resulting dataset 
(e.g. give the dimension, range of years, and names of key variables):

Description of data: 
These datasets were compiled by FiveThirtyEight to create an interactive tool exploring the relationship between political party control and economic performance, addressing concerns about "p-hacking" in scientific research. The `pols-month` dataset tracks U.S. political composition from 1947-2015, showing monthly counts of Democratic and Republican politicians across executive and legislative branches. The `snp` dataset contains S&P 500 closing values from 1950-2015, serving as a key indicator of stock market performance. The `unemployment` dataset provides monthly unemployment rates from 1948-2015, measuring labor market health. After cleaning and merging, the final dataset contains 822 observations spanning 68 years with 11 variables including year, month, presidential party affiliation ("gop" or "dem"), counts of politicians by party, S&P closing prices, and unemployment rates. This integrated dataset enables robust analysis of how political dynamics correlate with economic indicators while demonstrating how proper data management practices can mitigate p-hacking concerns by maintaining transparent, reproducible analytical processes.



## Problem 2

Importing and cleaning the Mr. Trash Wheel Data

```{r}
# Mr. Trash Wheel
mr_trash_wheel = read_excel("data/trashwheeldatanew.xlsx", 
  sheet = "Mr. Trash Wheel", skip = 1, na = c("NA", ".", "")) %>%  # Skip the first row (metadata/header),
  janitor:: clean_names() |>  # Clean column names and remove rows that don't have dumpster data (NA in dumpster column)
  filter(!is.na(dumpster))|>
  mutate(sports_balls = as.integer(round(sports_balls))) |> 
  select(dumpster:homes_powered)   # Select only the relevant columns with dumpster-specific data
mr_trash_wheel

# Professor Trash Wheel - same process with consistent data types
professor_trash_wheel = read_excel("data/trashwheeldatanew.xlsx", 
  sheet = "Professor Trash Wheel", skip = 1, na = c("NA", ".", "")) %>%  
  janitor::clean_names() |>  
  filter(!is.na(dumpster))|>
  # Ensure consistent data types
  mutate(
    year = as.character(year)  # Convert to character to match Mr. Trash Wheel
  ) |>
  select(dumpster:homes_powered)

# Gwynnda Trash Wheel - same process with consistent data types
gwynns_trash_wheel = read_excel("data/trashwheeldatanew.xlsx", 
  sheet = "Gwynns Falls Trash Wheel", skip = 1, na = c("NA", ".", "")) %>%  
  janitor::clean_names() |>  
  filter(!is.na(dumpster))|>
  # Ensure consistent data types
  mutate(
    year = as.character(year)  # Convert to character to match Mr. Trash Wheel
  ) |>
  select(dumpster:homes_powered)

# Add identifier variable to each dataset
mr_trash_wheel = mr_trash_wheel |> 
  mutate(trash_wheel = "Mr. Trash Wheel")

professor_trash_wheel = professor_trash_wheel |> 
  mutate(trash_wheel = "Professor Trash Wheel")

gwynns_trash_wheel = gwynns_trash_wheel |> 
  mutate(trash_wheel = "Gwynns Falls Trash Wheel")

all_trash_wheels = mr_trash_wheel |> 
  full_join(professor_trash_wheel) |> 
  full_join(gwynns_trash_wheel) |> 
  select(trash_wheel, everything())  # Move identifier to first column

# View the combined tidy dataset
all_trash_wheels
view(all_trash_wheels)

#Calculations requested for trash tons total and cigarette butts for June 2022
professor_total_weight <- all_trash_wheels |> 
  filter(trash_wheel == "Professor Trash Wheel") |> 
  summarise(total_weight_tons = sum(weight_tons, na.rm = TRUE))

gwynns_june_2022_butts <- all_trash_wheels |> 
  filter(trash_wheel == "Gwynns Falls Trash Wheel",
         year == "2022",
         month == "June") |> 
  summarise(total_cigarette_butts = sum(cigarette_butts, na.rm = TRUE))

# Display results
cat("Total weight of trash collected by Professor Trash Wheel:\n")
print(professor_total_weight)

cat("\nTotal number of cigarette butts collected by Gwynns in June 2022:\n")
print(gwynns_june_2022_butts)
nrow(all_trash_wheels)
```

Descriptionn of data: 
There are 1,188 total observations in the data of the combined three datasets. These data look at the trash collection from three different trash wheels which collect waste from waterways in Baltimore, Maryland. This data provides information on its key variables, such as the weight in tons of trash collected on certain days throughout the years up to 2025, how many plastic bottles were collected, cigarette_butts, glass bottles, plastic bags, wrappers, sports balls, and how many homes were powered by the trash wheel's trash collection. A few other variables are collected too. More rain can raise these numbers as it brings more trash in ans powers. the wheel.

282 tons was the total weight of trash collected by Professor Trash Wheel from the available data.

18,120 cigarette butts. was the total number of cigarette butts collected by Gwynnda in June of 2022.


## Problem 3
```{r}
zipcodes_df = read_csv("data/zillowdata/zipcodes.csv", na = c("NA", ".", "")) |>  
  janitor::clean_names() |> 
  select(
    -file_date,
    -county
  )

view(zipcodes_df)

zori_df = read_csv("data/zillowdata/zipzori.csv", na = c("NA", ".", "")) |>  
  janitor::clean_names() |> 
  pivot_longer(
    cols = starts_with("x20"),
    names_to = "date",
    values_to = "zillow_value"
  ) |> 
  mutate(
    date = format(ymd(gsub("x", "", date)), "%m/%d/%y")
  ) |> 
  select(
    zip_code = region_name,
    county = county_name,
    date,
    zillow_value,
    -region_id, -size_rank, -region_type, -state_name
  )

tidy_df <- zori_df |> 
  left_join(zipcodes_df, by = "zip_code") 
```

```{r}
tidy_df <- tidy_df |> #Adding borough variable to tidy_df
  mutate(
    borough = case_when(
      county == "New York County" ~ "Manhattan",
      county == "Kings County" ~ "Brooklyn", 
      county == "Queens County" ~ "Queens",
      county == "Bronx County" ~ "Bronx",
      county == "Richmond County" ~ "Staten Island",
      # Handle variations in county naming
      str_detect(tolower(county), "new york") ~ "Manhattan",
      str_detect(tolower(county), "kings") ~ "Brooklyn",
      str_detect(tolower(county), "queens") ~ "Queens", 
      str_detect(tolower(county), "bronx") ~ "Bronx",
      str_detect(tolower(county), "richmond") ~ "Staten Island",
      TRUE ~ "Other"  # For non-NYC counties
    )
  )
tidy_df <- tidy_df |>  # Create the borough column from county
  mutate(
    borough = case_when(
      str_detect(county, "New York") ~ "Manhattan",
      str_detect(county, "Kings") ~ "Brooklyn",
      str_detect(county, "Queens") ~ "Queens",
      str_detect(county, "Bronx") ~ "Bronx",
      str_detect(county, "Richmond") ~ "Staten Island",
      TRUE ~ "Other NYC Area"
    )
  )
view(tidy_df)
nrow(tidy_df) #Finding number of observations
dim(tidy_df)
```
Description of tidy_df: There are a total of 17, 516 observations. There are 43 unique neighborhoods. There are 149 unique zip codes. 

```{r}
# Check unique ZIP codes and neighborhoods
cat("Number of unique ZIP codes:", n_distinct(tidy_df$zip_code), "\n")
cat("Number of unique neighborhoods:", n_distinct(tidy_df$neighborhood), "\n")

# Find ZIP codes in zipcodes_df but not in zori_df
missing_zips <- zipcodes_df %>%
  anti_join(zori_df, by = "zip_code") %>%
  select(zip_code) %>%
  distinct()

cat("Number of ZIP codes in ZIP code dataset but not in Zillow dataset:", nrow(missing_zips), "\n")
```
There are 171 ZIP codes from the ZIP code dataset that do not appear in the Zillow dataset. Several factors explain why certain ZIP codes appear in the general ZIP code dataset but are excluded from Zillow's rental data. Primarily, Zillow requires sufficient market activity to generate reliable indices, which excludes rural areas with low population density and few rental properties. Military bases and government facilities with restricted housing markets are often omitted, as are PO box-only ZIP codes that lack residential addresses. Newly created ZIP codes or recent boundary changes may not yet be incorporated into Zillow's system. Additionally, areas with insufficient rental transaction data—including industrial parks, commercial districts, and large nature preserves—cannot support Zillow's statistical models and are consequently excluded from the dataset.
```{r}
#Creating table to find 10 biggest price drops with county and neighborhood
top_drops <- tidy_df %>%
  mutate(borough = case_when(
    str_detect(county, "New York") ~ "Manhattan",
    str_detect(county, "Kings") ~ "Brooklyn",
    str_detect(county, "Queens") ~ "Queens",
    str_detect(county, "Bronx") ~ "Bronx", 
    str_detect(county, "Richmond") ~ "Staten Island",
    TRUE ~ "Other NYC Area"
  )) %>%
  filter(date %in% c("01/31/20", "01/31/21")) %>%
  group_by(zip_code) %>% filter(n() == 2) %>%
  pivot_wider(names_from = date, values_from = zillow_value) %>%
  mutate(
    price_drop = `01/31/21` - `01/31/20`,
    percent_drop = ((`01/31/21` - `01/31/20`) / `01/31/20`) * 100
  ) %>%
  ungroup() %>% drop_na() %>%
  slice_min(price_drop, n = 10) %>%
  select(zip_code, borough, neighborhood, jan_2020 = `01/31/20`, jan_2021 = `01/31/21`, price_drop, percent_drop) %>%
  mutate(across(where(is.numeric), round, 2))
kable(top_drops)
```
The ten zip codes with the largest drop in price from January 2020 to 2021 are all in Manhattan and they include the neighborhoods of: Lower Manhattan, Lower East Side, Gramercy Park/Murray Hill, Chelsea/Clinton, and Greenwich Village/Soho. These zipcodes include 10007, 10009, 10016, 10001, 10002, 10004, 10038, 10012, 10010, and 10003.10007 provided the biggest price drop of $-912.60.














